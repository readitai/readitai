import os
import torch
import numpy as np
from scipy.io.wavfile import write

from PyTorch.SpeechSynthesis.Tacotron2.tacotron2 import model as tacotron2
from PyTorch.SpeechSynthesis.Tacotron2.tacotron2.text import text_to_sequence
from PyTorch.SpeechSynthesis.Tacotron2.waveglow import model as waveglow

from google.cloud import texttospeech
from google.oauth2 import service_account


__all__ = ['speak', 'speak_gwavenet', 'speak_tacotron2']


def checkpoint_from_distributed(state_dict):
    """
    Checks whether checkpoint was generated by DistributedDataParallel. DDP
    wraps model in additional "module.", it needs to be unwrapped for single
    GPU inference.
    :param state_dict: model's state dict
    """
    ret = False
    for key, _ in state_dict.items():
        if key.find('module.') != -1:
            ret = True
            break
    return ret


def unwrap_distributed(state_dict):
    """
    Unwraps model from DistributedDataParallel.
    DDP wraps model in additional "module.", it needs to be removed for single
    GPU inference.
    :param state_dict: model's state dict
    """
    new_state_dict = {}
    for key, value in state_dict.items():
        new_key = key.replace('module.1.', '')
        new_key = new_key.replace('module.', '')
        new_state_dict[new_key] = value
    return new_state_dict


def load_tacotron2(weightpath='checkpoints/tacotron2_20200314.pth'):

    ckpt = torch.load(weightpath)
    state_dict = ckpt['state_dict']
    if checkpoint_from_distributed(state_dict):
        state_dict = unwrap_distributed(state_dict)
    config = ckpt['config']

    tacotron2_model = tacotron2.Tacotron2(**config)
    tacotron2_model.load_state_dict(state_dict)
    tacotron2_model.text_to_sequence = text_to_sequence
    return tacotron2_model


def load_waveglow(weightpath='checkpoints/waveglow_20200314.pth'):

    ckpt = torch.load(weightpath)
    state_dict = ckpt['state_dict']
    if checkpoint_from_distributed(state_dict):
        state_dict = unwrap_distributed(state_dict)
    config = ckpt['config']

    waveglow_model = waveglow.WaveGlow(**config)
    waveglow_model.load_state_dict(state_dict)
    return waveglow_model


def speak_tacotron2(gib, speechparams, outputfn='speak_tmp_tacotron2.wav'):
    gib = [gib] if type(gib) == str else gib
    sent_batchsize = speechparams.sent_batchsize

    tacotron2_model = load_tacotron2()
    tacotron2_model = tacotron2_model.to('cuda')
    tacotron2_model.eval()

    waveglow_model = load_waveglow()
    waveglow_model = waveglow_model.remove_weightnorm(waveglow_model)
    waveglow_model = waveglow_model.to('cuda')
    waveglow_model.eval()

    audio_numpys = []
    for i, _ in enumerate(gib[::sent_batchsize]):
        text = ' '.join(gib[sent_batchsize * i: sent_batchsize * (i + 1)])
        print("Processing sentence %d" % (i*sent_batchsize))
        # preprocessing
        tacotron2_model.decoder.max_decoder_steps = 1e9
        sequence = np.array(tacotron2_model.text_to_sequence(text, ['english_cleaners']))[None, :]
        sequence = torch.from_numpy(sequence).to(device='cuda', dtype=torch.int64)

        # run the models
        with torch.no_grad():
            _, mel, _, _ = tacotron2_model.infer(sequence)
            try:
                audio = waveglow_model.infer(mel)
            except RuntimeError:
                print(' --- Sentence too long for my gpu. Skipping.')
                continue
        audio_numpy = audio[0].data.cpu().numpy()
        audio_numpys.append(audio_numpy)
    audio_numpy_all = np.concatenate(audio_numpys)

    print('Done converting. Writing to file.')
    write(outputfn, speechparams.rate, audio_numpy_all)


def speak(text):
    from gtts import gTTS

    tts = gTTS(text)
    tts.save('speak_tmp.mp3')


def speak_gwavenet(gib, speechparams, outputfn='speak_tmp.mp3', ssml=True):
    """Synthesizes speech from the input string of text."""
    gib = [gib] if type(gib) == str else gib
    sent_batchsize = speechparams.sent_batchsize

    credentials = service_account.Credentials.from_service_account_file(speechparams.credential)

    client = texttospeech.TextToSpeechClient(credentials=credentials)

    # Note: the voice can also be specified by name.
    # Names of voices can be retrieved with client.list_voices().
    voice = texttospeech.types.VoiceSelectionParams(
        language_code='en-US',
        name='en-US-Wavenet-C',
        ssml_gender=texttospeech.enums.SsmlVoiceGender.FEMALE)

    audio_config = texttospeech.types.AudioConfig(
        pitch=0,
        speaking_rate=1.0,
        audio_encoding=texttospeech.enums.AudioEncoding.MP3)

    # remove existing audio file to avoid appending to it
    if os.path.isfile(outputfn): os.remove(outputfn)

    for i, _ in enumerate(gib[::sent_batchsize]):
        text = ' '.join(gib[sent_batchsize * i: sent_batchsize * (i + 1)])
        if ssml:
            text = '<speak> ' + text + ' </speak>'
            input_text = texttospeech.types.SynthesisInput(ssml=text)
        else:
            input_text = texttospeech.types.SynthesisInput(text=text)
        response = client.synthesize_speech(input_text, voice, audio_config)

        # The response's audio_content is binary.
        with open(outputfn, 'ab') as out:
            out.write(response.audio_content)
            print('Finish writing sentence "{}"'.format(i))
